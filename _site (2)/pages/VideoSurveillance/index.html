
<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7 ie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8 ie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9 ie" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width,initial-scale=1">
	<title>Homemade Video Surveillance</title>
	<meta name="author" content="JH">
	<link href='/assets/themes/mytheme/css/style.css' rel="stylesheet" media="all">
	<link href="http://feeds.feedburner.com/" rel="alternate" title="Homemade Video Surveillance" type="application/atom+xml">
	<script src="http://cdnjs.cloudflare.com/ajax/libs/modernizr/2.0.6/modernizr.min.js"></script>
</head>
<body>

<div id="page" class="hentry">
	<header class="the-header">
		<div class="unit-head">
			<div class="unit-inner unit-head-inner">
				<a href="/index.html"><img src="/assets/images/common/hotglue_and_homemade_bits.png" height="100%" class="center" /></a>
			</div><!-- unit-inner -->
		</div><!-- unit-head -->
	</header>
	<div class="body" role="main">
		<div class="unit-body">
			<div class="unit-inner unit-body-inner">
				<div class="entry-content">
					
<article class="unit-article layout-page">
	<div class="unit-inner unit-article-inner">
		<div class="content">
			<header>
				<div class="unit-head">
					<div class="unit-inner unit-head-inner">
						<h1 class="h2 entry-title">Homemade Video Surveillance</h1>
					</div><!-- unit-inner -->
				</div><!-- unit-head -->
			</header>

			<div class="bd">
				<div class="entry-content">
					
<p>The aim of this project was to implement a low-power homemade video surveillance system using a cheap IP camera and a raspberry pi. There are several available frameworks/examples to achieve this, but…it is so much more interesting to build one from scratch, right ?. This has already been done numerous times, there is plenty of information over the internet and no real difficulty is involved. That is, if one chooses a decent IP camera, that can be accessed over HTTP as is the case for 99% of them. Unfortunately, I picked a cheap camera that uses a <em>proprietary</em> protocol, for which only a Windows client is available. Sigh…so I ended up spending most of the project time reverse engineering the camera protocol.<br /></p>

<ul id="markdown-toc">
  <li><a href="#hardware-parts" id="markdown-toc-hardware-parts">Hardware parts</a></li>
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#raspberry-pi-setup" id="markdown-toc-raspberry-pi-setup">Raspberry pi setup</a></li>
  <li><a href="#testing-the-audio-playback" id="markdown-toc-testing-the-audio-playback">Testing the audio playback</a></li>
  <li><a href="#ip-camera-setup" id="markdown-toc-ip-camera-setup">IP camera setup</a></li>
  <li><a href="#ip-camera-protocol" id="markdown-toc-ip-camera-protocol">IP camera protocol</a></li>
  <li><a href="#installing-opencv" id="markdown-toc-installing-opencv">Installing openCV</a></li>
  <li><a href="#motion-detection-algorithm" id="markdown-toc-motion-detection-algorithm">Motion detection algorithm</a></li>
  <li><a href="#the-surveillance-code" id="markdown-toc-the-surveillance-code">The surveillance code</a>    <ul>
      <li><a href="#video-files-management" id="markdown-toc-video-files-management">Video files management</a></li>
      <li><a href="#source-code" id="markdown-toc-source-code">Source code</a></li>
    </ul>
  </li>
  <li><a href="#installing-the-system" id="markdown-toc-installing-the-system">Installing the system</a></li>
  <li><a href="#misc-notes" id="markdown-toc-misc-notes">Misc notes</a>    <ul>
      <li><a href="#logging" id="markdown-toc-logging">Logging</a></li>
      <li><a href="#camera-viewing-angle" id="markdown-toc-camera-viewing-angle">Camera viewing angle</a></li>
      <li><a href="#security-disclaimer" id="markdown-toc-security-disclaimer">Security disclaimer</a></li>
      <li><a href="#remote-image-storage" id="markdown-toc-remote-image-storage">Remote image storage</a></li>
    </ul>
  </li>
  <li><a href="#lessons-learned" id="markdown-toc-lessons-learned">Lessons learned</a></li>
</ul>

<h3 id="hardware-parts">Hardware parts</h3>
<p>Anyway, here are the involved components:</p>

<ul>
  <li>an IP camera (in this case, a cheap model “InDoorCAM P2P” from Bluestork)
<img src="/assets/images/VideoSurveillance/bluestork_cam.png" alt="Bluestork cam" /></li>
  <li>a Raspberry pi (Model B+, but any model will do)</li>
  <li>a USB stick</li>
  <li>a wifi USB dongle (about 5$ at <a href="http://dx.com/p/dx-original-ultra-mini-usb-2-0-802-11n-b-g-150mbps-wi-fi-wlan-wireless-network-adapter-black-252716#.Uyx3FXX5PZg">DealExtreme</a>)</li>
  <li>a USB sound card for the raspberry pi (less than 3$ at <a href="http://dx.com/p/5-1-channel-usb-sound-card-adapter-blue-59037#.Uyx0enX5PZg">DealExtreme</a>)</li>
  <li>an audio amplifier module (around 5$ at <a href="http://www.dx.com/p/pam8403-dual-channel-amplifier-module-blue-216853#.U-Z2JeN_vFA">DealExtreme</a>)
<img src="/assets/images/VoiceControlledRadio/amplifier.png" alt="amplifier" /></li>
  <li>a speaker
<img src="/assets/images/VoiceControlledRadio/speaker.png" alt="speaker" /></li>
</ul>

<h3 id="overview">Overview</h3>

<p>In this project, the raspberry pi is entirely dedicated to capturing images from the camera and analyzing them. The camera is able to communicate over wifi, but to get maximum reliability, it will be connected to the raspi through an ethernet cable. Independently, a wifi dongle is used to communicate with the home router (e.g. to download images, administrate the system, etc…)</p>

<p>The system is connected as follows:
<img src="/assets/images/VideoSurveillance/global_schema.png" alt="system overview" /></p>

<p>Notes:</p>

<ul>
  <li>a powered USB hub is used, even though the Raspi model B+ has enough USB ports. This is because I ran into issues when powering a lot of stuff over the raspi’s USB in previous projects, so I don’t even take a chance anymore, an externally powered hub gives a more stable system.</li>
  <li>the use of a USB sound card is overkill here, the audio output of the pi would have been good enough, but I initially wanted to have the capability to record sound too.</li>
  <li>the USB stick will be used (exclusively) to store live images when image detection is triggered.</li>
</ul>

<h3 id="raspberry-pi-setup">Raspberry pi setup</h3>
<p>I installed a default Raspbian distribution from raspberrypi.org</p>

<p>1) transfer raspbian image to SD card:</p>

<pre><code>sudo dd bs=1M if=XXX-raspbian.img of=/dev/xxx
</code></pre>

<p>2) plug-in a mouse/keyboard/HDMI display and boot-up</p>

<p>3) Use <code>raspi-config</code> to configure the raspberry as required (e.g. keyboard layout)</p>

<p>4) Plug wi-fi dongle, boot to graphical environment, configure wifi settings.</p>

<p>5) Plug &amp; configure USB sound card (which is required to get sound input, as it is not available on the raspberry board itself). In my case it boiled down to:</p>

<pre><code>sudo nano /etc/modprobe.d/alsa-base.conf
</code></pre>

<p>change:</p>

<pre><code>options snd-usb-audio index=-2
</code></pre>

<p>into:</p>

<pre><code>options snd-usb-audio index=0
</code></pre>

<p>6) Surprisingly, I had a hard time setting up the network config to have both wifi and ethernet <em>simultaneously</em> working. By default, whenever I plugged an ethernet cable, the wifi interface would be shut down. The following changes were required to achieve the desired setup:</p>

<p>Edit <code>/etc/network/interfaces</code></p>

<pre><code>auto lo
iface lo inet loopback
auto eth0
allow-hotplug eth0
iface eth0 inet static
address 192.168.1.1
netmask 255.255.255.0

auto wlan0
allow-hotplug wlan0
iface wlan0 inet manual
wpa-roam /etc/wpa_supplicant/wpa_supplicant.conf
iface default inet dhcp
</code></pre>

<p>Check the content of <code>/etc/default/ifplugd</code></p>

<pre><code>INTERFACES="auto"
HOTPLUG_INTERFACES="all"
ARGS="-q -f -u0 -d10 -w -I"
SUSPEND_ACTION="stop"
</code></pre>

<p>Edit <code>/etc/rc.local</code> to add:</p>

<pre><code># Disable the ifplugd eth0
sudo ifplugd eth0 --kill
sudo ifup wlan0
</code></pre>

<p>7) Prepare mount point for USB disk</p>

<pre><code>sudo mkdir /mnt/imagestore
sudo chmod a+w imagestore
</code></pre>

<p>8) Setup auto-mount of USB disk at boot time, adding in /etc/fstab:</p>

<pre><code>/dev/sda1 /mnt/imagestore ext4 defaults 0 0
</code></pre>

<p>Finally reboot the pi:</p>

<pre><code>sudo reboot
</code></pre>

<h3 id="testing-the-audio-playback">Testing the audio playback</h3>

<p>Checking sound output :</p>

<pre><code>aplay &lt;any wav file&gt;
</code></pre>

<h3 id="ip-camera-setup">IP camera setup</h3>

<p>I proceeded as follows to configure the IP of the Bluestork camera:</p>

<ul>
  <li>connect to the camera over wifi, using the provided Windows client</li>
  <li>In the “LAN” tab, right-click the camera to adjust IP address (192.168.1.2 in my case)</li>
</ul>

<p>Note : since my local wifi network is 192.168.0.xxxx, after this setup, the camera cannot be accessed directly over wifi anymore.</p>

<p>This camera delivers 640x480 images, at up to (theoretically) 30 images per second, but I configured it to send 3 images/sec, which is enough for my need (and ensures that my image analysis code keeps up easily)</p>

<p>An interesting tip: by default the camera comes with a Telnet server installed, and the login/passwd is “admin/admin” (sigh…) 
This revealed a few interesting things, like the UDP ports the camera is using:</p>

<pre><code>
	~ # netstat -l
	Active Internet connections (only servers)
	Proto Recv-Q Send-Q Local Address           Foreign Address         State
	tcp        0      0 :::23                   :::*                    LISTEN
	udp        0      0 0.0.0.0:5000            0.0.0.0:*
	udp        0      0 0.0.0.0:2473            0.0.0.0:*
	udp        0      0 0.0.0.0:2627            0.0.0.0:*
	udp        0      0 0.0.0.0:11760           0.0.0.0:*
	udp        0      0 0.0.0.0:8694            0.0.0.0:*
	Active UNIX domain sockets (only servers)
	Proto RefCnt Flags       Type       State         I-Node Path

	/bin # cat /proc/cpuinfo
	system type             : Ralink SoC
	processor               : 0
	cpu model               : MIPS 24K V4.12
	BogoMIPS                : 239.61
	wait instruction        : yes
	microsecond timers      : yes
	tlb_entries             : 32
	extra interrupt vector  : yes
	hardware watchpoint     : yes
	ASEs implemented        : mips16 dsp
	VCED exceptions         : not available
	VCEI exceptions         : not available
</code></pre>

<p>User config files happen to be stored in <code>/mnt/spinand/sif</code>, and specifically:</p>

<pre><code>/mnt/spinand/sif # cat hkclient.conf
wifiopen=0
USER=&lt;redacted&gt;
LANPORT=5000
PORT=8080
Alias=IPCAM
VERSION=41
UpdateUrl=http://www.uipcam.com/app/experience/download.jsp?fileName=bsd_6360.txt
WANENABLE=0
PROXY=bluestork.scc21.net
LogLevel=0
PASSWD=n4qpj7
LogBackground=1
CLIENT=hkipcame
/mnt/spinand/sif # http://www.uipcam.com/app/experience/download.jsp?fileName=bsd_6360.txtCommitRemoteHostInfo   1
CommitRemoteHostInfo   2
CommitRemoteHostInfo   1
CommitRemoteHostInfo   2
</code></pre>

<h3 id="ip-camera-protocol">IP camera protocol</h3>

<p>The IP camera model I chose unfortunately does not support standard (e.g. HTTP) communication to retrieve images, but uses a proprietary protocol based on UDP, implemented by a Windows client. So I ended up reverse engineering just enough of the protocol to be able to start the image acquisition and get the image data. Using the Windows client with Wireshark running in the background, and after countless hours of head scratching and trial&amp;error tests, I isolated a packet sequence fulfilling my need (though most of the details of the protocol remain obscure, at least I got the d***** thing to work). The sequence goes as follows:<br /></p>

<pre><code>* Initialization sequence:
  * initialize an UDP socket, bind it to any available port on the local pc
  * All UDP packets mentioned below will be sent to the camera's IP address on port 5000 
  * send 43 bytes (constant data as retrieved from Wireshark logs)
  * send 13 bytes (template data as retrieved from Wireshark logs + random value in 7th byte)
    * receive 13 bytes in return
    * the 5th byte should have its bit4 set, and the 7th byte should not have had its bit6 set
    * if this is not the case, retry until this is the case
  * send another 13 bytes (constant data as retrieved from Wireshark logs)
  * send another 13 bytes (constant data as retrieved from Wireshark logs)
    * get 13 bytes in return
  * send 212 bytes (constant data as retrieved from Wireshark logs)
    * receive 155 bytes in return
  * send 34 bytes
  * send 119 bytes
    * receive 368 bytes in return
* At this point the image data packet will start coming
* The first image data packet has a 15 bytes header, followed by the JPEG StartOfImage pattern (0xffd8)
* subsequent packets have a 4 bytes header, the first byte being a simple sequence number incremented at each packet
* one of the subsequent packets will have an EndOfImage pattern (0xffd9) in its data, which completes the reception of one frame.
* NOTE: after a	few images, the end of image pattern does not necessarily come as the last two bytes of a fragment, but can be in the middle of a packet, immediately followed by the beginning of the data for the next frame.
* to sustain the image flow, every few packets received, a control packet needs to be sent, with most of the data taken from wireshark capture, and the last bytes updated dynamically, based on a predefined sequence (0xd9,0xd8,0xdb,0xda,0xdd,0xdc,0xdf,0xde,0xd1,0xd0) and subtle variations over time. Check the code for details, this part was tricky)
</code></pre>

<p><strong><em>Side note</em></strong>:<br />
Rik Bobbaers pointed out an interesting lead to go further in this reverse engineering exercise: the <a href="http://www.mrsafe.be/?portfolio=wireless-hd-ip-camera">MrSafe HD Indoor camera</a> is apparently similar to the Bluestork model I own, and comes with an Android app (get it <a href="http://www.mrsafe.be/wp-content/uploads/2015/01/mrsafe-0522.apk">here</a>) that could maybe be reverse-engineered, more easily than the Bluestork windows app:<br /></p>

<ul>
  <li>unzip the apk and get the <code>classes.dex</code> file, which can be unpacked with <a href="https://code.google.com/p/dex2jar/">dex2jar</a>:</li>
</ul>

<pre><code>./d2j-dex2jar classes.dex</code></pre>

<ul>
  <li>
    <p>a Java decompiler like <a href="http://jd.benow.ca/">this one</a> could then be used to help figuring out the protocol: launch <code>JD-GUI</code>, open the <code>classes.dex</code> file, and there you go, the java classes source code.<br /></p>
  </li>
  <li>
    <p>In addition, <a href="http://code.google.com/p/android-apktool/">apktool</a> can be used to extract info from the original apk file, especially to get the unencrypted content of all XML files</p>
  </li>
</ul>

<pre><code>apktool d mrsafe-0522.apk</code></pre>

<p>In directory <code>/x1/Studio/Ali</code>, <code>Start.java</code> and <code>Video.java</code> look interesting. Mentions of devices supporting P2P protocol in the code is also a hint (though I already tried some generic P2P client to connect to my camera, and failed). This reverse engineering effort is to be continued for now…just for fun, since the current code is good enough anyway to get everything working as far as I am concerned.</p>

<h3 id="installing-opencv">Installing openCV</h3>

<p>I will be using openCV for implementing the image analysis. To install it on the raspberry pi, first check the raspi repo database is up to date :</p>

<pre><code>sudo apt-get update
</code></pre>

<p>Then just install openCV:</p>

<pre><code>sudo apt-get install libopencv-dev python-opencv
</code></pre>

<p><code>ffmpeg</code> is required to save images as compressed video:</p>

<pre><code>sudo apt-get install ffmpeg
</code></pre>

<h3 id="motion-detection-algorithm">Motion detection algorithm</h3>

<p>A very simple differential images algorithm is used, to keep performance requirements to a minimum.</p>

<ul>
  <li>at all times, keep the latest two captured images (N-2 and N-1) in memory</li>
  <li>capture a new image (N)</li>
  <li>convert it to grayscale</li>
  <li>compute the absolute difference (in grayscale) between N and N-2, and between N and N-1.</li>
  <li>compute the bitwise AND of these two delta images</li>
  <li>apply a threshold on the resulting image to get a binary result (pixel below threshold at 0, pixels above threshold at 255)</li>
  <li>count the number of pixels at 255.</li>
  <li>a detection is notified if this number exceeds a predefined threshold.</li>
</ul>

<p>Then it is all a matter of carefully choosing the various settings/threshold to minimize false positive while keep a decent sensitivity. Experimentation is key, since it depends a lot on the type of scene being captured.</p>

<h3 id="the-surveillance-code">The surveillance code</h3>

<p>An easy way to start experimenting with image capture early during the project was to use a python script to send/receive packets. I initially planned to move to a C implementation later now for performance concerns, but it turns out that for my limited need (capturing/analysing/saving a few low-res images per second), Python running on the raspi is just fine performance-wise.</p>

<p>The script therefore ended up doing this:</p>

<ul>
  <li>send the magic packets previously identified to start image flow</li>
  <li>loop (until a socket error occurs) doing this:
    <ul>
      <li>read incoming packets</li>
    </ul>
  </li>
  <li>accumulate enough packets to have (probably) enough data for a whole image</li>
  <li>if enough data is available, search the data for the jpeg “start of image” and “end of image” markers
    <ul>
      <li>if the markers are found, isolate the JPEG data bytes, and start image analysis:
        <ul>
          <li>read the data as an openCV image</li>
          <li>apply the image analysis described above. If motion is detected:
            <ul>
              <li>in this case, save the N-2, N-1, and N images, and trig the capture of the next few images too.</li>
              <li>also play a beep sound on the audio output upon detection.</li>
            </ul>
          </li>
          <li>in any case, shift the N-2/N-1/N images for next iteration.</li>
        </ul>
      </li>
      <li>every few fragments received, compute and send the “continue” packet, as per the reverse-engineering protocol.</li>
    </ul>
  </li>
  <li>if a socket error occurs (cheap camera…), restart at the beginning.</li>
</ul>

<p>As an example, it captured a random cat passing by:
<img src="/assets/images/VideoSurveillance/capture_cat.png" alt="cat capture" /></p>

<p>And the associated binary image that triggered this particular capture:
<img src="/assets/images/VideoSurveillance/capture_cat_binary.png" alt="cat capture binary" /></p>

<p><strong>Note</strong>: After a while, I noticed that some of the detections were triggered by the plant in the lower left corner, whenever it was slightly moving with the wind. To avoid this, I added a mask in the motion detection algorithm, with this part of the image masked out. The mask is as follows:</p>

<p><img src="/assets/images/VideoSurveillance/detectionmask_bottomleft.png" alt="detection mask" /></p>

<p>Note that due to the specific image orientation of my setup, the mask image is flipped horizontally and vertically.</p>

<h4 id="video-files-management">Video files management</h4>
<p>The script also:</p>

<ul>
  <li>opens a new capture file everyday, in which all motion detections for that day will be concatenated. This allows to quickly review the surveillance images for a given day by opening a single file</li>
  <li>dumps the binary image corresponding to each detection, for debug purposes.</li>
  <li>unconditionnally dumps a single image every 15 minutes, as a background monitoring</li>
</ul>

<p>A cron rule is in charge of cleaning up old capture files, deleting the oldest ones (older than 4 days in this example):</p>

<pre><code>0 0 * * * date \&amp;\&amp; find /mnt/imagestore -mtime +4 -exec rm -rf {} \; &gt;&gt; /mnt/imagestore/purge.log 2&gt;\&amp;1
</code></pre>

<h4 id="source-code">Source code</h4>
<p>The full python script is available <a href="https://github.com/jheyman/videosurveillance">here</a>. Note that the hardcoded byte sequences apply to my camera, you would have to capture your own data with Wireshark. This is horrible python code, it started as a simple hack to play with camera settings, and ended up being the base for the final version since I got fed up after finally completing the reverse engineering part, and did not have the courage to clean it up.</p>

<h3 id="installing-the-system">Installing the system</h3>

<p>The final installation in its weather-proof box is show below. The top cover of the box contains the USB hub, the USB storage, the audio, and the wi-fi dongle:</p>

<p><img src="/assets/images/VideoSurveillance/assembly_cover.png" alt="system internal view cover" /></p>

<p>Note : the small PCB with screw connectors contains a chip that is unused in this project, I just happened to reuse this PCB from another project for convenience purposes (see <a href="/pages/VoiceControlledRadio">here</a>)<br />
The other side of the box contains the Raspberry pi and the main power supply:</p>

<p><img src="/assets/images/VideoSurveillance/assembly_box.png" alt="system internal view box" /></p>

<h3 id="misc-notes">Misc notes</h3>

<h4 id="logging">Logging</h4>

<p>When executing the python script as a background job and redirecting its execution output to a logfile, I noticed that none of the print statements seemed to end up in the log file. It turns out that by default, python buffers the outputs and they might not end up in the log before a while. To ensure that all outputs appear in the log file as soon as they come, I used the <code>-u</code> (unbuffered) python option. The complete line to start the script in the background while also capturing its output is:</p>

<pre><code>python -u surveillance.py &gt; surveillance.log 2&gt;&amp;1 &amp;
</code></pre>

<h4 id="camera-viewing-angle">Camera viewing angle</h4>
<p>The IP camera I bought has a limited viewing angle. Since it is mounted quite near the front door, I was not able to view everything I needed. I just bought a super cheap chinese wide-angle lens for smartphones (a few dollars), and considering that I don’t care about image quality, this did the job.</p>

<p><img src="/assets/images/VideoSurveillance/fish_eye.png" alt="fish eye" /></p>

<h4 id="security-disclaimer">Security disclaimer</h4>

<p>There is no security management whatsoever. The reason is simple: the camera only captures images of my front door, which is visible from the street anyway. I just don’t care about the camera being hacked. I would NEVER deploy such an unprotected system for an indoor camera, where very strong privacy/security concerns apply.</p>

<h4 id="remote-image-storage">Remote image storage</h4>

<p>After a while, I decided to store images remotely (on a NAS) instead of locally on the USB stick, for two reasons:</p>

<ul>
  <li>I had two problems 6 months apart where somehow the file system on the USB stick got corrupted. In retrospect, using a USB stick as a permanent image storage media is not a very wise decision.</li>
  <li>it’s safer this way, the images now being stored directly inside the house, I will still have access to them if something bad (…) happens to the outdoor camera box.</li>
</ul>

<p>Anyway, I changed one line in <code>etc/fstab</code> to mount a NAS shared folder over NFS, instead of the USB stick:</p>

<pre><code>192.168.0.12:/volume1/backup/videosurveillance /mnt/imagestore nfs defaults,user,auto,noatime 0 0	
</code></pre>

<h3 id="lessons-learned">Lessons learned</h3>

<ul>
  <li>OpenCV is AWESOME and even simpler to use than I anticipated, especially in Python.</li>
  <li>Python is good enough for (slow) real-time image analysis, even on a raspi.</li>
  <li>Properly filtering out false positives in image motion detection algorithm is tricky.</li>
</ul>


				</div><!-- entry-content -->

				<br>
				<hr>
				<div class="misc-content">			
					


  <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_developer = 1;
    var disqus_shortname = 'jheymantechblog'; // required: replace example with your forum shortname
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




				</div><!-- misc-content -->				
			</div><!-- bd -->

			<footer class="unit-foot">
				<div class="unit-inner unit-foot-inner">
					<p class="gotop">
						<a href="#page">Back to Top</a>
					</p>
				</div>
			</footer>

		</div><!-- content -->
	</div><!-- unit-inner -->
</article>


				</div>
			</div><!-- unit-inner -->
		</div><!-- unit-body -->
	</div><!-- body -->
	<footer class="the-footer">
		<div class="unit-foot">
			<div class="unit-inner unit-foot-inner">
				<div class="misc vcard">
					<h4>about</h4>
					<ul>
						<li class="contact"><address><span class="author fn n">JH</span> - <span class="fn email">bidsomail at gmail.com</span></address></li>
						<li>Powered by <a href="https://github.com/mojombo/jekyll">Jekyll</a>, theme based on the_minimum from <a href="http://jekyllbootstrap.com/">Jekyll-bootstrap</a></li>
					</ul>
				</div><!-- misc -->
			</div><!-- unit-foot-inner -->
		</div><!-- unit-foot -->
	</footer>
</div><!-- page -->
<script>
	(function(d, s) {
		var js, fjs = d.getElementsByTagName(s)[0], load = function(url, id) {
		if (d.getElementById(id)) {return;}
		js = d.createElement(s); js.src = url; js.id = id;
		fjs.parentNode.insertBefore(js, fjs);
		};
	load('//platform.twitter.com/widgets.js', 'tweetjs');
	// load('https://apis.google.com/js/plusone.js', 'gplus1js'); // Checkout http://j.mp/ApDgMr for usage html for this is <div class="g-plusone" data-size="medium"></div>
	// load('//connect.facebook.net/en_US/all.js#xfbml=1', 'fbjssdk'); // Checkout http://j.mp/wZw2xR for using open graph protorol html for this is <div class="fb-like" data-href="/pages/VideoSurveillance/" data-send="false" data-layout="button_count" data-width="450" data-show-faces="false" data-font="verdana"></div>
	}(document, 'script'));
</script>
<script>
/*! A fix for the iOS orientationchange zoom bug.Script by @scottjehl, rebound by @wilto. MIT License.*/
(function(j){var i=j.document;if(!i.querySelectorAll){return}var l=i.querySelectorAll("meta[name=viewport]")[0],a=l&&l.getAttribute("content"),h=a+", maximum-scale=1.0",d=a+", maximum-scale=10.0",g=true,c=j.orientation,k=0;if(!l){return}function f(){l.setAttribute("content",d);g=true}function b(){l.setAttribute("content",h);g=false}function e(m){c=Math.abs(j.orientation);k=Math.abs(m.gamma);if(k>8&&c===0){if(g){b()}}else{if(!g){f()}}}j.addEventListener("orientationchange",f,false);j.addEventListener("deviceorientation",e,false)})(this);
</script>

  


  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-43264312-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>



</body>
</html>

